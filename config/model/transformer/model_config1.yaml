train:
  epochs: 10
  steps: 500          # For printing training loss.
  batch_size: 16      # 16 is good for distributed/multiGPU setup. Use lower if on single GPU with less VRAM
  learning_rate: 0.0001
  optimizer: "adamw"
  half_precision: False
model:
  type: "transformer"
  d_model: 512        # The dimensionality of the output before the classifier layer.
  n_layers: 3
  n_heads: 4
  max_len: 2304       # 2048 + 256 (longest mel spec length in LibriSpeech is 2185).
  n_mels: 80