train:
  epochs: 500
  steps: 500          # For printing training loss.
  batch_size: 8      # 16 is good for distributed/multiGPU setup. Use lower if on single GPU with less VRAM
  learning_rate: 0.001 # 0.01
  optimizer: "adamw"
  recon_loss: "mse"   # mse or l1
  half_precision: False  # Use fp16 or not (float32)
model:
  d_model: 256
  n_enc_layer: 3
  n_dec_layer: 3
  vocab_size: 45
  d_state: 32
  dt_rank: "auto"
  d_conv: 4
  pad_vocab_size_multiple: 8
  conv_bias: true
  bias: false
  scan_mode: "cumsum" #"logcumsumexp" # "cumsum" or "logcumsumexp"
  n_mels: 80